{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb52391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114f820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b07c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::808242303800:role/sagemaker-execution-role\n",
      "sagemaker bucket: sagemaker-us-east-1-808242303800\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "s3_prefix = 'hf-small-tune'\n",
    "\n",
    "# train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/hf_data/train'\n",
    "\n",
    "# test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/hf_data/test'\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n",
    "#filepath = f\"s3://sagemaker-us-east-1-808242303800/bert-wiki-mlops/raw_dataset\"\n",
    "train_image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04'\n",
    "inference_image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bcda50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/data-process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/data-process.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument(\"--file-name\", type=str)\n",
    "    parser.add_argument(\"--model-name\", type=str)\n",
    "    parser.add_argument(\"--train-ratio\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--val-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--star-threshold\", type=int, default=3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # read data\n",
    "#    s3 = S3FileSystem() \n",
    "#    input_data_path =\"/opt/ml/processing/input\"\n",
    "#    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    squad = load_dataset(\"squad\")\n",
    "#    squad_s3 = load_from_disk(input_data_path)\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=384,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            answer = answers[i]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "        \n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "            \n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "        return inputs\n",
    "    \n",
    "    tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    default_bucket = 'sagemaker-us-east-1-808242303800'\n",
    "    s3_prefix = 'hf-small-tune'\n",
    "    s3 = S3FileSystem() \n",
    "    \n",
    "    # save train_dataset to s3\n",
    "    training_input_path = f's3://{default_bucket}/{s3_prefix}/hf_data/train'\n",
    "    tokenized_squad[\"train\"].shuffle().select(range(30000)).save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "    # save test_dataset to s3\n",
    "    test_input_path = f's3://{default_bucket}/{s3_prefix}/hf_data/test'\n",
    "    tokenized_squad[\"validation\"].shuffle().select(range(5000)).save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86548a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/train.py\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, Trainer, TrainingArguments, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=16)\n",
    "    parser.add_argument(\"--eval-batch-size\", type=int, default=16)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=2e-5)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\"INFO\"),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "    test_dataset = load_from_disk(args.test_dir)\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "    # download model from model hub\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(args.model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        num_train_epochs=args.epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9589626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iam_helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iam_helper.py\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "\n",
    "def create_lambda_role(role_name):\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(\n",
    "                {\n",
    "                    \"Version\": \"2012-10-17\",\n",
    "                    \"Statement\": [\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "                            \"Action\": \"sts:AssumeRole\",\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            Description=\"Role for Lambda to call SageMaker functions\",\n",
    "        )\n",
    "\n",
    "        role_arn = response[\"Role\"][\"Arn\"]\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\", RoleName=role_name\n",
    "        )\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f\"Using ARN from existing role: {role_name}\")\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        return response[\"Role\"][\"Arn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aec67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/lambda_sync.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/lambda_sync.py\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\" \"\"\"\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # The name of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\"Created Endpoint!\"),\n",
    "        \"other_key\": \"example_value\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09455f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/lambda_async.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/lambda_async.py\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\" \"\"\"\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # The name of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    bucket = event[\"bucket\"]\n",
    "    s3_prefix = event[\"s3_prefix\"]\n",
    " \n",
    "    container = {\"ModelPackageName\": model_package_arn}\n",
    "    create_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n",
    "\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "        AsyncInferenceConfig={\n",
    "            \"OutputConfig\": {\n",
    "                \"S3OutputPath\": f\"s3://{bucket}/{s3_prefix}/output\",\n",
    "            },\n",
    "            \"ClientConfig\": {\n",
    "                \"MaxConcurrentInvocationsPerInstance\": 4\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\"Created Endpoint!\"),\n",
    "        \"other_key\": \"example_value\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5763cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_processor = ScriptProcessor(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "#    instance_type='local_gpu',\n",
    "#    transformers_version='4.6',\n",
    "#    pytorch_version='1.7',\n",
    "    instance_count=1,\n",
    "    command=['python3'],\n",
    "#    sagemaker_session=PipelineSession(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7495a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"HuggingFaceDataProcessStep\",\n",
    "    processor=sp_processor,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--model-name\",\n",
    "        \"distilbert-base-uncased\",\n",
    "        \"--train-ratio\",\n",
    "        \"0.7\",\n",
    "        \"--val-ratio\",\n",
    "        \"0.15\",\n",
    "        \"--star-threshold\",\n",
    "        \"4\"\n",
    "    ],\n",
    "    code='scripts/data-process.py',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444ca87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_uri = train_image_uri\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={\n",
    "    'epochs': 1,\n",
    "    'model_name': 'distilbert-base-uncased'\n",
    "}\n",
    "\n",
    "huggingface_estimator = Estimator(\n",
    "        entry_point='scripts/train.py',\n",
    "        image_uri=train_image_uri,\n",
    "        output_path=f's3://{sess.default_bucket()}/{s3_prefix}/train',  \n",
    "        code_location=f's3://{sess.default_bucket()}/{s3_prefix}/train',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        # transformers_version='4.12.3',\n",
    "        # pytorch_version='1.9.1',\n",
    "        # py_version='py38',\n",
    "        hyperparameters=hyperparameters,\n",
    "        disable_profiler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb161937",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"hf-train\",\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=training_input_path,\n",
    "            # content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=test_input_path,\n",
    "            # content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "step_train.add_depends_on([step_process])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "124681aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:258: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = inference_image_uri\n",
    "# Create Model\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "model = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    step_args=model.create(\"ml.g4dn.xlarge\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8b031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"Approved\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"hf-small-model\",\n",
    "    estimator=huggingface_estimator,\n",
    "    image_uri = inference_image_uri,\n",
    "    #model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    #model=\"s3://sagemaker-us-east-1-808242303800/hf-small-tune/train/pipelines-m199adkqjuew-hf-train-Ux3gOnEEFx/output/model.tar.gz\",\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.g4dn.xlarge\"],\n",
    "    # transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"hf-model-group\",\n",
    "    approval_status=model_approval_status,\n",
    "    # model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac79fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARN from existing role: lambda-deployment-role\n"
     ]
    }
   ],
   "source": [
    "from iam_helper import create_lambda_role\n",
    "\n",
    "lambda_role = create_lambda_role(\"lambda-deployment-role\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc5f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current time to define unique names for the resources created\n",
    "import time\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "model_name = \"sync-hf-model\" + current_time\n",
    "endpoint_config_name = \"sync-hf-endpoint-config\" + current_time\n",
    "endpoint_name = \"sync-hf-endpoint-\" + current_time\n",
    "function_name = \"sagemaker-demo-sync-lambda-step\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"scripts/lambda_sync.py\",\n",
    "    handler=\"lambda_sync.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(output_name=\"other_key\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "# in the Lambda\n",
    "step_sync_deploy_lambda = LambdaStep(\n",
    "    name=\"LambdaStepHuggingFaceSyncDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": step_create_model.properties.ModelName,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e519ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current time to define unique names for the resources created\n",
    "import time\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "model_name = \"async-hf-model\" + current_time\n",
    "endpoint_config_name = \"async-hf-endpoint-config\" + current_time\n",
    "endpoint_name = \"async-hf-endpoint-\" + current_time\n",
    "function_name = \"sagemaker-async-hf-lambda-step\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"scripts/lambda_async.py\",\n",
    "    handler=\"lambda_async.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(output_name=\"other_key\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "# in the Lambda\n",
    "step_async_deploy_lambda = LambdaStep(\n",
    "    name=\"LambdaStepHuggingFaceAsyncDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": step_register.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": role,\n",
    "        \"bucket\": sagemaker_session_bucket,\n",
    "        \"s3_prefix\": s3_prefix\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0de4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"hf3-sagemaker-workshop-pipeline\",\n",
    "    parameters=[\n",
    "        model_approval_status,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model, step_register, step_sync_deploy_lambda, step_async_deploy_lambda],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f580ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:808242303800:pipeline/hf3-sagemaker-workshop-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '24558466-79af-4268-a9c7-cfdd1ce8a34f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '24558466-79af-4268-a9c7-cfdd1ce8a34f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Sat, 19 Nov 2022 11:32:04 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.create(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3209e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f011c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Body': <botocore.response.StreamingBody object at 0x7fbca9c40e20>,\n",
      " 'ContentType': 'application/json',\n",
      " 'InvokedProductionVariant': 'AllTraffic',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '65',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Sat, 19 Nov 2022 13:25:25 GMT',\n",
      "                                      'x-amzn-invoked-production-variant': 'AllTraffic',\n",
      "                                      'x-amzn-requestid': '21b47df8-a3aa-4867-8338-00f8cb6dd361'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '21b47df8-a3aa-4867-8338-00f8cb6dd361',\n",
      "                      'RetryAttempts': 0}}\n",
      "{'score': 0.7042312622070312, 'start': 11, 'end': 16, 'answer': 'Clara'}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "endpoint_name = \"sync-hf-endpoint-11-19-11-32-02\"\n",
    "content_type = \"application/json\"\n",
    "payload = {\n",
    "\t'inputs': {\n",
    "\t\t\"question\": \"What's my name?\",\n",
    "\t\t\"context\": \"My name is Clara and I live in Berkeley.\"\n",
    "\t}\n",
    "}\n",
    "res = json.dumps(payload)\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=res\n",
    ")\n",
    "\n",
    "pp.pprint(response)\n",
    "response_json = json.loads(botocore.response.StreamingBody.read(response['Body']))\n",
    "print(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0a1f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker.Session().boto_region_name\n",
    "import json\n",
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sm_client\n",
    ")\n",
    "\n",
    "item = {\"inputs\":{\n",
    "\t\t\"question\": \"What's my name?\",\n",
    "\t\t\"context\": \"My name is Clara and I live in Berkeley.\"\n",
    "\t}}\n",
    "\n",
    "with open(\"test.json\", 'w') as f:\n",
    "    f.write(json.dumps(item))\n",
    "    \n",
    "s3_item = sagemaker_session.upload_data(path = 'test.json', bucket = sess.default_bucket(), key_prefix = s3_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d44a8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_runtime = boto_session.client(\"sagemaker-runtime\")\n",
    "response_1 = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=\"async-hf-endpoint-11-19-11-32-03\",\n",
    "    InputLocation=s3_item,\n",
    "    ContentType=\"application/json\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98c0b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\":0.26507049798965454,\"start\":11,\"end\":39,\"answer\":\"Clara and I live in Berkeley\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "out_prefix = 'hf-small-tune/output/'\n",
    "out_file = re.search('output\\/(.*)$', str(response_1['OutputLocation'])).group(1)\n",
    "sagemaker_session.download_data(path = './test_output', bucket = sess.default_bucket(), key_prefix = out_prefix + out_file)\n",
    "with open('./test_output/' + os.listdir('./test_output')[0]) as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2f36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
